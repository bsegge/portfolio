{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Perceptron with Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "y_names = iris.target_names\n",
    "y_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be classifying the iris dataset as \"setosa\" (1) and \"not setosa\" (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classes = []\n",
    "for i in y:\n",
    "    if i == 0:\n",
    "        new_classes.append(1)\n",
    "    else:\n",
    "        new_classes.append(0)\n",
    "y = np.array(new_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_perceptron(input_vector, weights):\n",
    "    \"\"\"\n",
    "    Perceptron prediction function that will return a positive or negative classification\n",
    "    \n",
    "    input: \n",
    "        input_vector: numpy array; vector of independent variables\n",
    "        weights: iterable array object; repspective weights per input variable\n",
    "    output:\n",
    "        activation: binary; positive or negative classification\n",
    "    \"\"\"\n",
    "    # insert 1 to account for bias (1*bias in weight vector will have no effect)\n",
    "    input_vector = np.insert(input_vector,0,1)\n",
    "    # summation of input vector * weights\n",
    "    dot_prod = np.dot(input_vector, weights)\n",
    "    # if activation > 0 than postive classification, otherwise negative classification\n",
    "    activation = 1 if dot_prod > 0 else 0\n",
    "    return activation\n",
    "    \n",
    "def train_perceptron(input_vector, weights, labels, epochs=100, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Perceptron training function that adjusts the weights and bias terms\n",
    "    \n",
    "    input:\n",
    "        input_vector: numpy array; matrix or numpy array of independent variables\n",
    "        weights: list; list of weights, one per independent variable plus 1 bias term\n",
    "        labels: list; the correct labels for the data\n",
    "        epochs: int; the number of iterations you'd like to train\n",
    "        learning_rate: float; the amount to \"learn\" each iteration\n",
    "    output:\n",
    "        weights: list; list of trained weights for the perceptron\n",
    "    \"\"\"\n",
    "    for _ in range(epochs):\n",
    "        for inp, lab in zip(input_vector, labels):\n",
    "            pred = predict_perceptron(inp, weights)\n",
    "            weights[1:] += learning_rate * (lab - pred) * inp\n",
    "            weights[0] += learning_rate * (lab - pred)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 0.3800000000000001, 1.2400000000000002, -1.6400000000000001, -0.7]\n"
     ]
    }
   ],
   "source": [
    "# train weights on train set\n",
    "\n",
    "# set weights to 0 to start (including bias term)\n",
    "init_weights = [0 for i in range(len(X[0])+1)]\n",
    "init = [X_train, init_weights, y_train, 1000, 0.2]\n",
    "weights = train_perceptron(*init)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        69\n",
      "           1       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n",
      "                       pred not-versicolor  pred versicolor\n",
      "actual not-versicolor                   69                0\n",
      "actual versicolor                        0               31\n"
     ]
    }
   ],
   "source": [
    "# test accuracy on training set\n",
    "\n",
    "predictions = []\n",
    "for row in X_train:\n",
    "    pred = predict_perceptron(row, weights)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "class_report = classification_report(y_train, predictions)\n",
    "conf_report = confusion_matrix(y_train, predictions)\n",
    "print(class_report)\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        conf_report\n",
    "        , columns=['pred not-versicolor','pred versicolor']\n",
    "        , index=['actual not-versicolor','actual versicolor']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        31\n",
      "           1       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "                       pred not-versicolor  pred versicolor\n",
      "actual not-versicolor                   31                0\n",
      "actual versicolor                        0               19\n"
     ]
    }
   ],
   "source": [
    "# test accuracy on test set\n",
    "\n",
    "predictions = []\n",
    "for row in X_test:\n",
    "    pred = predict_perceptron(row, weights)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "class_report = classification_report(y_test, predictions)\n",
    "conf_report = confusion_matrix(y_test, predictions)\n",
    "print(class_report)\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        conf_report\n",
    "        , columns=['pred not-versicolor','pred versicolor']\n",
    "        , index=['actual not-versicolor','actual versicolor']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Conclusion: The Perceptron did a perfect job at classifying setosa iris'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer NN with Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NOTE: Professor, I tried hard to code this from scratch, and I followed several blog posts closely, but I'm afraid its just not quite clicked for me. I have to admit defeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function\n",
    "def sigmoid(x, deriv=False):\n",
    "    if deriv == True:\n",
    "        return x * (1 - x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# forward propogation\n",
    "def forward(inputs, weights):\n",
    "    return sigmoid(np.dot(inputs, weights))\n",
    "\n",
    "# back propogation\n",
    "def backprop(inputs, hidden, labels, weights):\n",
    "    error = labels - hidden\n",
    "    delta = error * sigmoid(hidden, deriv=True)\n",
    "    weights += np.dot(inputs.T, delta)\n",
    "    return weights\n",
    "\n",
    "# train nn\n",
    "def train_nn(inputs, init_weights, labels, epochs=10000):\n",
    "    hidden = forward(inputs, init_weights)\n",
    "    weights = init_weights\n",
    "    for _ in range(epochs):\n",
    "        weights = backprop(inputs, hidden, labels, weights)\n",
    "        hidden = forward(inputs, weights)\n",
    "    return weights\n",
    "    \n",
    "# predict new\n",
    "def predict_nn(inputs, trained_weights):\n",
    "    pred = sigmoid(np.dot(inputs, weights))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_weights = np.array([0.0,0.0,0.0,0.0,0.0])\n",
    "inputs = np.hstack((np.array(np.ones(100)).reshape(100,1),X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Round 2...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resource: https://towardsdatascience.com/neural-network-on-iris-data-4e99601a42c8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_input(X, y, hidden=6):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        X: numpy array\n",
    "        y: numpy array\n",
    "    output:\n",
    "        W1, W2: initial random weights for input layer and hidden layer\n",
    "        b1, b2: initial zeroed bias terms for input and hidden layer\n",
    "    \"\"\"\n",
    "    hidden_layer_size = hidden\n",
    "    # W1 = np.zeros(( hidden_layer_size, X_train.shape[1] ))\n",
    "    W1 = np.random.randn( hidden_layer_size, X.shape[1] ) * 0.01\n",
    "    b1 = np.zeros(( hidden_layer_size, 1 ))\n",
    "    # W2 = np.zeros(( y_train[1], hidden_layer_size ))\n",
    "    W2 = np.random.randn( 1, hidden_layer_size ) * 0.01\n",
    "    b2 = np.zeros(( 1, 1))\n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation and tanh functions for hidden and output layers\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, W1, W2, b1, b2):\n",
    "    # dot prod of first layer\n",
    "    dp1 = np.dot(W1, X.T) + b1\n",
    "    # activation of first layer\n",
    "    act1 = tanh(dp1)\n",
    "    # dot prod of second layer\n",
    "    dp2 = np.dot(W2, act1) + b2\n",
    "    # activation of second layer\n",
    "    act2 = sigmoid(-dp2)\n",
    "    return dp1, dp2, act1, act2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y, W1, W2, act2):\n",
    "    W1=W1\n",
    "    W2=W2\n",
    "    m = y.shape[0]\n",
    "\n",
    "    logprobs = np.multiply(np.log(act2), y) + np.multiply((1 - y), np.log(1 - act2))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(X, y, W1, W2, act1, act2):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    dW2 = (1 / m) * np.dot((act2-y), act1.T)\n",
    "    db2 = (1 / m) * np.sum((act2-y), axis=1, keepdims=True)\n",
    "    dZ1 = (np.multiply(np.dot(W2.T, (act2-y)), 1 - np.power(act1, 2)))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    return dW1, dW2, db1, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(W1, W2, dW1, dW2, b1, b2, db1, db2, learning_rate=0.1):\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, y, epochs=1000):\n",
    "    W1, W2, b1, b2 = define_input(X, y)\n",
    "    for _ in range(epochs):\n",
    "        dp1, dp2, act1, act2 = forward_prop(X, W1, W2, b1, b2)\n",
    "#         cost = cost(y, W1, W2, act2)\n",
    "        dW1, dW2, db1, db2 = backprop(X, y, W1, W2, act1, act2)\n",
    "        W1, W2, b1, b2 = update_weights(W1, W2, dW1, dW2, b1, b2, db1, db2)\n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, b1, b2 = nn_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 4.52892280e-30, 1.00000000e+00, 4.52892962e-30,\n",
       "        1.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dp1, dp2, act1, act2\n",
    "forward_prop(X_train[0], W1, W2, b1, b2)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
